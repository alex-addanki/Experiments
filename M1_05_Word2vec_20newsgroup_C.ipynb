{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_05_Word2vec_20newsgroup_C.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-addanki/Experiments/blob/master/M1_05_Word2vec_20newsgroup_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH62CWxTDTpf",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj",
        "colab_type": "text"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  Preprocess text data\n",
        "*  Apply word2vec to find the Representation of  text document \n",
        "*  Classify the word2vec represented documents using KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRbZDcS2qArZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Experiment Walkthrough\n",
        "#@markdown Word2vec representation of Newsgroup text\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n",
        "  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/JAN20/word2vec_experiment.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pobf8O4bBRJ",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "In this experiment we use the 20 newsgroup dataset\n",
        "\n",
        "**Description**\n",
        "\n",
        "This dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. That is there are approximately one thousand documents taken from each of the following newsgroups:\n",
        "\n",
        "    alt.athesim\n",
        "    comp.graphics   \n",
        "    comp.os.ms-windows.misc\n",
        "    comp.sys.ibm.pc.hardware\n",
        "    comp.sys.mac.hardware\n",
        "    comp.windows.x\n",
        "    misc.forsale\n",
        "    rec.autos\n",
        "    rec.motorcycles\n",
        "    rec.sport.baseball\n",
        "    rec.sport.hockey\n",
        "    sci.crypt\n",
        "    sci.electronics\n",
        "    sci.med\n",
        "    sci.space\n",
        "    soc.religion.christian\n",
        "    talk.politics.guns\n",
        "    talk.politics.mideast\n",
        "    talk.politics.misc\n",
        "    talk.religion.misc\n",
        "\n",
        "The dataset consists of **Usenet** posts--essentially an email sent by subscribers to that newsgroup. They typically contain quotes from previous posts as well as cross posts i.e. a few posts may be sent to more than once in a newsgroup.\n",
        "\n",
        "Each newsgroup is stored in a subdirectory, with each post stored as a separate file.\n",
        "\n",
        "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
        "\n",
        "We have slight modified the original dataset above and made the same available to you as a pickle file (To understand more about pickle look at Python_Pickle_Introduction.ipynb experiment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZYYZDRUDZQe",
        "colab_type": "text"
      },
      "source": [
        "### Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3kyR-_DYJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5pX0FDNDfCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY5RpQ9ZbWAM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook=\"M1W1_05_Word2vec_20newsgroup_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx pip3 install gensim\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_NEWSGROUPS_PICKELFILE.pkl\")\n",
        "    #ipython.magic(\"sx unzip AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip?dl=1\")\n",
        "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n",
        "    ipython.magic(\"sx unzip AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n",
        "    ipython.magic(\"sx wget https://www.dropbox.com/s/9xivz2pox1i83td/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1\")\n",
        "    ipython.magic(\"sx mv AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1 AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\")\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    \n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      print(\"Your submission is successful.\")\n",
        "      print(\"Ref Id:\", submission_id)\n",
        "      print(\"Date of submission: \", r[\"date\"])\n",
        "      print(\"Time of submission: \", r[\"time\"])\n",
        "      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n",
        "      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if Additional: return Additional      \n",
        "    else: raise NameError('')\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "    from IPython.display import HTML\n",
        "    HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id))\n",
        "  \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCwSJmXNDTpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required Packages\n",
        "import pickle\n",
        "import operator\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import collections\n",
        "import gensim\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkOdbWcbDTpm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urLEOmkbDTpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pickle_dataset = pickle.load(open('AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n",
        "print(pickle_dataset.keys())\n",
        "\n",
        "# Print frequencies of dataset\n",
        "print(\"Class : count\")\n",
        "print(\"--------------\")\n",
        "number_of_documents = 0\n",
        "for key in pickle_dataset:\n",
        "    print(key, ':', len(pickle_dataset[key]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1md10WDTpy",
        "colab_type": "text"
      },
      "source": [
        "## Splitting into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0AYtzi-DTpz",
        "colab_type": "text"
      },
      "source": [
        "Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n",
        "\n",
        "As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. \n",
        "\n",
        "\n",
        "To know about decode refer the below link :\n",
        "\n",
        "https://www.tutorialspoint.com/python/string_decode.htm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjDQqmbMDTp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = {}\n",
        "test_set = {}\n",
        "decoded_dataset = {}\n",
        "# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n",
        "for key in pickle_dataset:\n",
        "    decoded_dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in pickle_dataset[key]]\n",
        "    \n",
        "# Break dataset into 95-5 split for training and testing\n",
        "n_train = 0\n",
        "n_test = 0\n",
        "for k in decoded_dataset:\n",
        "    split = int(0.95*len(decoded_dataset[k]))\n",
        "    train_set[k] = decoded_dataset[k][0:split]\n",
        "    test_set[k] = decoded_dataset[k][split:]\n",
        "    n_train += len(train_set[k])\n",
        "    n_test += len(test_set[k])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ayx2e-FOmf",
        "colab_type": "text"
      },
      "source": [
        "In the following cell/block, we are finding the frequency of the words and storing it in a dictionary. By doing this we have the vocabulary built for the corpus (in this case 20 news groups)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcXuwTGWDTp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a dictionary to store frequencies of words.\n",
        "# Key:Value === Word:Count\n",
        "frequency = defaultdict(int)\n",
        "    \n",
        "for key in train_set:\n",
        "    for f in train_set[key]:\n",
        "       # print(type(f))\n",
        "        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n",
        "        # We ignore all special characters such as !.$ and words containing numbers\n",
        "        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n",
        "    \n",
        "        for word in words:\n",
        "            frequency[word] += 1\n",
        "\n",
        "sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
        "print(\"Top-10 most frequent words:\")\n",
        "for word in sorted_words[:10]:\n",
        "    print(word)\n",
        "\n",
        "print('----------------------------')\n",
        "print(\"10 least frequent words:\")\n",
        "for word in sorted_words[-10:]:\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvDVamv_FGH_",
        "colab_type": "text"
      },
      "source": [
        "### Ignore the 25 most frequent words, and the words which appear less than 100 times\n",
        "\n",
        "Please observe the bar chart of word vs its frequency. Generally the stop words, the words that appear on most of the documents and the words that are too rare come in this list. These words don't add much value in the classification task. So, we are removing them to decrease the vocabulary. Removing the words in vocabulary helps in saving memory and speeds up the processing (i.e. it decreases the space complexity to large extent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJpqahqtDTp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_words = defaultdict(int)\n",
        "\n",
        "print('Number of words before preprocessing:', len(sorted_words))\n",
        "\n",
        "ignore_most_frequent = 25\n",
        "freq_thresh = 100\n",
        "feature_number = 0\n",
        "for word, word_frequency in sorted_words[ignore_most_frequent:]:\n",
        "    if word_frequency > freq_thresh:\n",
        "        valid_words[word] = feature_number\n",
        "        feature_number += 1\n",
        "        \n",
        "print('Number of words after preprocessing:', len(valid_words))\n",
        "\n",
        "word_vector_size = len(valid_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXqK59gGDTqE",
        "colab_type": "text"
      },
      "source": [
        "# 1. Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csm5a0hrDTqI",
        "colab_type": "text"
      },
      "source": [
        "In the previous section we saw a naive way to represent words as dense vectors which can leverage the semantics of the words.\n",
        "\n",
        "The problem with count-based word representations is that they are costly in terms of memory to compute large co-occurrence matrices. Let us see another method to find representations of words without explicitly counting words.\n",
        "\n",
        "Here, we aim to predict the next word given the context in which the word appears. (For example, given the last $n$ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors.\n",
        "\n",
        "Link to pretrained 300 dimensional word2vec: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QBnlIZxDTqK",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Visualizations\n",
        "\n",
        "Before we go to the actual 300 dimensional vectors, let's try to explore some of the more intriguing properties of word2vec.\n",
        "\n",
        "You have been provided with a sample of word vectors. **We have reduced the dimensionality of the 300-dimensional vectors to 2 dimensions, so that we can plot them in matplotlib.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg0FXnBhDTqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_values(values, labels, figsize = (8,4), c = []):\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=figsize) \n",
        "    for i in range(len(labels)):\n",
        "        plt.scatter(x[i],y[i], color=c[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import pickle\n",
        "two_dim_model = pickle.load(open('AIML_DS_WORD2VEC2D_STD.pkl', 'rb'))\n",
        "\n",
        "wv_labels = {}\n",
        "for vec, word in two_dim_model:\n",
        "    wv_labels[word] = vec\n",
        "    \n",
        "colors = ['blue' for i in range(len(wv_labels))]\n",
        "\n",
        "plot_values(wv_labels.values(), list(wv_labels.keys()), figsize = (16, 9), c = colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Yxw_7FDTqP",
        "colab_type": "text"
      },
      "source": [
        "Now, we have given you the 2D representation of different word vectors. Plot the word vectors for the words 'King', 'Queen', 'man', 'women', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest' in green color:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMF6vO05DTqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_list = ['king', 'queen', 'man', 'woman', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest']\n",
        "wv_new_labels = {}\n",
        "for word in wv_list:\n",
        "    wv_new_labels[word] = wv_labels[word]\n",
        "\n",
        "colors = ['green' for i in range(len(wv_new_labels))]\n",
        "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PbuN6OaDTqU",
        "colab_type": "text"
      },
      "source": [
        "Consider the word analogy question: \"Queen is to King, as Woman is to what?\"\n",
        "\n",
        "To answer this question, we aim to find what the difference between a \"King\" and \"Queen\" is, and apply that difference to a \"Woman\". If we try to put this mathematically, we can write:- \n",
        "$$\n",
        " Answer = Woman + King - Queen\n",
        "$$\n",
        "\n",
        "Compute the value of the vector on the right hand side of the above equation and plot the resulting vector in red in the same plot as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHer2vrbDTqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = wv_new_labels['woman']  + wv_new_labels['queen'] - wv_new_labels['king']\n",
        "\n",
        "wv_new_labels['answer1'] = answer\n",
        "\n",
        "colors = ['green' if word not in ['answer1'] else 'red' for word in wv_new_labels]\n",
        "\n",
        "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kCh_NClDTqY",
        "colab_type": "text"
      },
      "source": [
        "Notice how the answer vector is very close to the vector of the word \"Man\"? Incidentally, \"Man\" is the right answer to the word analogy question! This is the power of Word2Vec representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsODt3Z9DTqa",
        "colab_type": "text"
      },
      "source": [
        "### Ungraded Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47AFoEUeDTqd",
        "colab_type": "text"
      },
      "source": [
        "Using this technique answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpoeXWV9DTqf",
        "colab_type": "text"
      },
      "source": [
        "* Germany is to France, as Paris is to?\n",
        "* Best is to Good, as Strongest is to?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19D8C40N2kC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOcxKzuvDTql",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Load pre-trained Word2Vec\n",
        "\n",
        "Let us now proceed to load the complete pretrained vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXoawLkSDTqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFdjtmZLDTqq",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Word2Vec representation\n",
        "\n",
        "Convert each document into average of the word2vec vectors of all valid words in document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRvcxruXH1Kx",
        "colab_type": "text"
      },
      "source": [
        " This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time, we store the same noise vector for training and test time in substitute_word_vecs variable.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzo0mCdIYYb",
        "colab_type": "text"
      },
      "source": [
        "###  Now, let us think about what we mean by \"valid vector for representing a word\" and why it doesn't exist in some cases?\n",
        "\n",
        "* It happens when we have  misspelt word in the vocabulary\n",
        "* If the vocabulary for corpus which you are classifying and the corpus used to find the vector embedding are different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiZxIihmDTqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_vector_size = 300\n",
        "def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n",
        "    labels = np.zeros((number_of_documents, 1))\n",
        "    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n",
        "    \n",
        "    # Iterate over the dataset and split into words\n",
        "    i = 0\n",
        "    for label, class_name in enumerate(dataset):\n",
        "        for f in dataset[class_name]:\n",
        "            text = ' '.join(f).split(' ')\n",
        "            valid_count = 1\n",
        "            for word in text:\n",
        "                \n",
        "                # Check if word is valid or not according to original dataset pruning\n",
        "                if word in valid_words:\n",
        "                    try:\n",
        "                        w2v_rep[i] += model[word]\n",
        "                    except:\n",
        "                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n",
        "                         to account for this. We store the random vector we assigned to the word, and reuse \n",
        "                         the same vector during test time to ensure consistency.'''\n",
        "                        \n",
        "                        if word not in substitute_word_vecs.keys():\n",
        "                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n",
        "                            \n",
        "                        w2v_rep[i] += substitute_word_vecs[word]\n",
        "                    \n",
        "                    valid_count += 1\n",
        "            \n",
        "            # Average\n",
        "            w2v_rep[i] = w2v_rep[i] / valid_count\n",
        "            \n",
        "            # Save label\n",
        "            labels[i] = label\n",
        "            \n",
        "            i += 1\n",
        "    \n",
        "    return w2v_rep, labels, substitute_word_vecs\n",
        "\n",
        "# Convert the train and test datasets into their word2vec representations\n",
        "train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n",
        "test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrs_bVWhDTqy",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Document classification using Word2Vec\n",
        "\n",
        "You may try to understand what distance metric Sklearn's KNeighborsClassifier uses in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhGbqmGYWX3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUD0G5QWmls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(train_w2v_set, train_w2v_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wlk0vKXW3Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_value = neigh.predict(test_w2v_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpsGiz45XQ6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyk-FTCiXSBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(test_w2v_labels,predicted_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_rLlG_zDTq5",
        "colab_type": "text"
      },
      "source": [
        "### Ungraded Exercise 2\n",
        "\n",
        "The frequency thresholds represents the minimum frequency a word must have to be considered relevant. Experiment with the following values of frequency threshold in your preprocessing step from section 1.2. Re-run all the codes with the new set of valid words and report the accuracies. Use the following values:\n",
        "\n",
        "`freq_thresh` = \n",
        "* 10\n",
        "* 1000\n",
        "\n",
        "Report the accuracies using word2vec features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN9_U292DTq-",
        "colab_type": "text"
      },
      "source": [
        "### Ungraded Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcurZoWaDTq_",
        "colab_type": "text"
      },
      "source": [
        "In section 1.3, substitute_word_vectors is used as a proxy for a word vector which we do not know. We used a normal gaussian to represent this in that section. Experiment with the type of substitute word vectors used when you do not have a pretrained word vecto is used as the following:\n",
        "\n",
        "`substitute_word_vecs` : \n",
        "* np.ones\n",
        "* np.zeros\n",
        "\n",
        "Report the accuracies using only word2vec features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX-DgEQ42sFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot2965O2DTrD",
        "colab_type": "text"
      },
      "source": [
        "### Ungraded Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGBzFigADTrE",
        "colab_type": "text"
      },
      "source": [
        "To classify news articles into their 20 news groups, experiment with three different parameter values with the following parameter choices.\n",
        "\n",
        "* K-NN \n",
        " ** K : 10, 50\n",
        " ** Distance Metric : Euclidean.\n",
        "\n",
        "\n",
        "Report the accuracies using  word2vec features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFFxKvDH2vCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YIha6yMtyuu",
        "colab_type": "text"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLMuQ3eOnHQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title How does the Word2Vec algorithm solve the general problem of context-specific meaning? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"It uses a broad context window to solve the problem.\",\"Word2Vec doesn't solve the problem of context-specific meaning\",\"By using many dimensions to represent each word (e.g., 600 dimensions) context can be accounted for.\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7IGEVYztuqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W8f70N9t1ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKI_65_Zt31W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"Yes\", \"No\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1RNSXbVuKcP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id =return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}